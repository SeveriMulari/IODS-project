# Week 2, analyzing 

*Describe the work you have done this week and summarize your learning.*

# 1. There is 7 variables and 166 observations in the data set. Variables: gender, age, attitude, deep, stra, surf, points. Deep, stra, surf were modified during data wrankling and points=0 were excluded.

# 2. Graphical overview and summaries
  - you can see from the plots, that when attitude gets higher on x-axis, the points get higher on y-axis. Points are quite similar among female and male. Green are male and red are female, as you can see from plot matrix where are used ggpairs, there are some differences among sex. 
  
# 3. Regression model:
 Call:
lm(formula = points ~ attitude + stra + surf, data = learning2014)

Residuals:
     Min       1Q   Median       3Q      Max 
-17.1550  -3.4346   0.5156   3.6401  10.8952 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  11.0171     3.6837   2.991  0.00322 ** 
attitude     33.9522     5.7415   5.913 1.93e-08 ***
stra          0.8531     0.5416   1.575  0.11716    
surf         -0.5861     0.8014  -0.731  0.46563    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 5.296 on 162 degrees of freedom
Multiple R-squared:  0.2074,	Adjusted R-squared:  0.1927 
F-statistic: 14.13 on 3 and 162 DF,  p-value: 3.156e-08

# so as you can see from the summary of my fitted model, attitude and strategic learning affected to points. Estimates (attitude 33.9522 and strategic learning 0.8531). Surface learning affected negatively to the points (estimate -0.5861). There are NO statistical significance in strategic learning (p=0.11716) and surface learning (p=0.46563). But in attitude, there are statistical significance, p < 0.05. So i excluded deep and strategic learning from the model and made new:

Call:
lm(formula = points ~ attitude, data = learning2014)

Residuals:
     Min       1Q   Median       3Q      Max 
-16.9763  -3.2119   0.4339   4.1534  10.6645 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   11.637      1.830   6.358 1.95e-09 ***
attitude      35.255      5.674   6.214 4.12e-09 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 5.32 on 164 degrees of freedom
Multiple R-squared:  0.1906,	Adjusted R-squared:  0.1856 
F-statistic: 38.61 on 1 and 164 DF,  p-value: 4.119e-09

# now in this simple regression model, there are statistical significance. Points are target variable and attitude is explanatory variable. So to summarise: better attitude gives better points! R-squared = Explained variation /total variation. So r-squared is the percentage of the response variable variation that is explained by a linear model. Always between 0% and 100%, 0% means that the model explains none of the variability and 100% means that model explains all the variability.

# assumptions of the diagnostic plots: 
  - Normal Q-Q plots fall nicely to the line, so the errors of the model are normally distributed (which is good!)
  - Residuals vs leverage plots can help identify which observations have an usual high impact. In my residuals vs leverage plots, there are regular data errors. Or maybe there could be one outlier... 
  - Residuals vs fitted; size of the error should not depend on the explanatory variables. My residuals vs fitted model seems pretty reasonable to my eye...   
 

#used codes

# graphs (tasks 1 and 2)
install.packages("ggplot2")
library(ggplot2)

p1 <- ggplot(learning2014, aes(x = attitude, y = points, col = gender))

p2 <- p1 + geom_point()
 
p2 (drawing plots)

p3 <- p2 + geom_smooth(method = "lm") (regression line)

p4 <- p3 + ggtitle("Student's attitude versus exam points") (TITLE)

p4 (plot)

pairs(learning2014[-1], col = learning2014$gender) (scatter plot matrix)

install.packages("GGally")
library(GGally)

p <- ggpairs(learning2014, mapping = aes(col = gender, alpha = 0.3), lower = list(combo = wrap("facethist", bins = 20))) (more advanced plot matrix, using ggpairs)

p (drawing advanced plot matrix)

# regression model 

ggpairs(learning2014, lower = list(combo = wrap("facethist", bins = 20))) (plot matrix)

my_model2 <- lm(points ~ attitude + stra + surf, data = learning2014) (regression model with multiple explanatory variables, attitude and strategic learning and surface learning)

summary(my_model2) (printing my model)

NO statistical significance in strategic or deep learning so i make new model!

my_model11 <- lm(points ~ attitude, data = learning2014)

summary(my_model11) (printing out new model)

# diagnostic plots
par(mfrow = c(2,2)) (4 graphics to the same plot)
plot(my_model2, which = c(1,2,5)) (printing plots)
