data <- read.csv("C:\\Users\\sever\\OneDrive\\Desktop\\IODS-project\\data\\create_alc, header = TRUE)
data <- read.csv("C:\\Users\\sever\\OneDrive\\Desktop\\IODS-project\\data\\create_alc, header = TRUE)
data
data
data <- read.csv("C:\\Users\\sever\\OneDrive\\Desktop\\IODS-project\\data\\create_alc, header = TRUE, sep = " ")
data
data
data <- read.table("C:\\Users\\sever\\OneDrive\\Desktop\\IODS-project\\data\\create_alc, header = TRUE, sep = " ")
data <- read.table("C:\\Users\\sever\\OneDrive\\Desktop\\IODS-project\\data\\create_alc, header = TRUE, sep = " ")
data
data
data <- read.table("C:\\Users\\sever\\OneDrive\\Desktop\\IODS-project\\data\\create_alc, header = TRUE)
data <- read.table("C:\\Users\\sever\\OneDrive\\Desktop\\IODS-project\\data\\create_alc, header = TRUE")
write.csv(alc, file = ALC_Excel)
write.csv(alc, file = ALC_Excel)
write.table(alc, file = "create_alc11")
create_alc <- read.csv("C:/Users/sever/OneDrive/Desktop/IODS-project/data/create_alc.R", sep=";", comment.char="#")
View(create_alc)
View(alc)
View(alc)
data <- read.table("C:\Users\sever\OneDrive\Desktop\IODS-project\data\create_alc", header = TRUE)
data <- read.table("C:\\Users\\sever\\OneDrive\\Desktop\\IODS-project\\data\\create_alc", header = TRUE)
View(data)
dim(data)
colnames(data)
summary(data)
hist(data$age)
hist(data$sex)
hist(data$sex)
hist(data$guardian)
hist(data$health)
alc %>% group_by(sex, high_use) %>% summarise(count = n(), mean_grade = mean(G3))
summary(data$age)
summary(data$sex)
summary(data$guardian)
summary(data$health)
install.packages("gmodels")
library(gmodels)
g1 <- ggplot(alc, aes(x = high_use, y = G3, col = sex))
g1 + geom_boxplot() + ylab("grade")
g2 <- ggplot(alc, aes(x = high_use, y = absences, col = sex))
g2 + geom_boxplot() + ggtitle("Student absences by alcohol consumption and sex")
crosstab(data$age, data$high_use)
crosstable(data$age, data$high_use)
CrossTable(data$age, data$high_use)
CrossTable(data$sex, data$high_use)
CrossTable(data$guardian, data$high_use)
CrossTable(data$health, data$high_use)
median(age)
median(data$age)
CrossTable(median(data$age), data$high_use)
model <- glm(high_use ~ age + guardian + sex + health, data = data, family = "binomial")
model
summary(model)
OR <- coef(model) %>% exp
OR <- coef(model) %>% exp
CI <- confint(model) %>% exp
cbind(OR, CI)
model <- glm(high_use ~ age + guardian + sex + health, data = data, family)
model
summary(model)
# Age and sexM had p-values < 0.05 and have thus statistical significance.
OR <- coef(model) %>% exp
CI <- confint(model) %>% exp
cbind(OR, CI)
model <- glm(high_use ~ age + sex + guardian + health, data = data)
model
summary(model)
# Age and sexM had p-values < 0.05 and have thus statistical significance.
OR <- coef(model) %>% exp
CI <- confint(model) %>% exp
cbind(OR, CI)
probabilities <- predict(model, type = "response")
data <- mutate(data, probability = probabilities)
data <- mutate(data, prediction = probability > 0.5)
select(data, age, sex, guardian, health, high_use, probability, prediction) %>% tail(10)
table(high_use = data$high_use, prediction = data$prediction)
View(g1)
View(two_columns)
probabilities <- predict(model, type = "response")
data <- mutate(data, probability = probabilities)
data <- mutate(data, prediction = probability > 0.5)
select(data, age, sex, high_use, probability, prediction) %>% tail(10)
table(high_use = data$high_use, prediction = data$prediction)
OR <- coef(model) %>% exp
CI <- confint(model) %>% exp
cbind(OR, CI)
install.packages("MASS")
library(MASS)
data(Boston)
str(Boston)
dim(Boston)
summary(Boston)
install.packages("corrplot")
library(corrplot)
cor_matrix <- cor(Boston)
corrplot(cor_matrix, method="circle", type="upper")
corrplot
corrplot(cor_matrix, method="circle", type="upper")
View(cor_matrix)
Boston_scaled <- scale(Boston)
summary(Boston_scaled)
Boston_scaled <- as.data.frame(Boston_scaled)
summary(Boston_scaled$crim)
bins <- quantile(Boston_scaled$crim)
crime <- cut(Boston_scaled$crim, breaks = bins, include.lowest = TRUE, labels = c("low", "med_low", "med_high", "high"))
Boston_scaled <- dplyr::select(Boston_scaled, -crim)
dim(Boston_scaled)
Boston_scaled <- data.frame(Boston_scaled, crime)
dim(Boston_scaled)
str(Boston_scaled)
nrow(Boston_scaled)
n <- nrow(Boston_scaled)
ind <- sample(n, size = n*0.8)
train <- Boston_scaled[ind, ]
test <- Boston_scaled[-ind, ]
lda.fit <- lda(crime ~ ., data = train)
lda.fit
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "orange", tex = 0.75, choices = c(1,2)){
heads <- coef(x)
arrows(x0 = 0, y0 = 0,
x1 = myscale * heads[,choices[1]],
y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
text(myscale * heads[,choices], labels = row.names(heads),
cex = tex, col=color, pos=3)
}
classes <- as.numeric(train$crime)
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1)
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "orange", tex = 0.75, choices = c(1,2)){
heads <- coef(x)
arrows(x0 = 0, y0 = 0,
x1 = myscale * heads[,choices[1]],
y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
text(myscale * heads[,choices], labels = row.names(heads),
cex = tex, col=color, pos=3)
}
classes <- as.numeric(train$crime)
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1)
View(lda.fit)
correct_classes <- test$crime
test <- dplyr::select(test, -crime)
lda.pred <- predict(lda.fit, newdata = test)
table(correct = correct_classes, predicted = lda.pred$class)
library(MASS)
data(Boston)
Boston2 <- Boston
Boston2_scaled <- scale(Boston2)
dist_eu <- dist(Boston2_scaled)
summary(dist_eu)
km <- kmeans(Boston2_scaled, centers = 4)
pairs(Boston2_scaled, col = km$cluster)
k_max <- 10
twcss <- sapply(1:k_max, function(k){kmeans(Boston, k)$tot.withinss})
twcss <- sapply(1:k_max, function(k){kmeans(Boston, k)$tot.withinss})
twcss <- sapply(1:k_max, function(k){kmeans(Boston, k)$tot.withinss})
install.packages("ggplot2")
library(ggplot2)
qplot(x = 1:k_max, y = twcss, geom = "line")
p1
km <- kmeans(Boston2_scaled, centers = 2)
pairs(Boston2_scaled, col = km$cluster)
library(MASS)
data(Boston)
Boston3 <- Boston
Boston3_scaled <- scale(Boston3)
Boston3_scaled <- data.frame(Boston3_scaled)
km3 <- kmeans(Boston3_scaled, centers = 4)
lda.fit3 <- lda(km3$cluster ~., data = Boston3_scaled)
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "orange", tex = 0.75, choices = c(1,2)){
heads <- coef(x)
arrows(x0 = 0, y0 = 0,
x1 = myscale * heads[,choices[1]],
y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
text(myscale * heads[,choices], labels = row.names(heads),
cex = tex, col=color, pos=3)
}
plot(lda.fit3, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit3, myscale = 1)
View(lda.arrows)
<Anonymous> ... <Anonymous> -> parse_all -> parse_all.character -> parse
knitr::opts_chunk$set(echo = TRUE)
# Here is exercise 4; clustering and classification analysis exercise!
# 2) Let's first download the Boston data:
install.packages("MASS")
library(MASS)
data(Boston)
str(Boston)
dim(Boston)
#The dataset includes 506 observations and 14 variables. The data contains housing data from the Boston area, including variables such as crime rate (crim), distance to employment centers (dis), and pupil-teacher ratio (ptratio).
# 3) Let's have a look at a summary of the included variables:
summary(Boston)
#The summary suggests that crime rates range from 0.00632 to 88.97620 per capita (IQR 0.08-3.67), and that the pupil-teacher ratio ranges from 12.6 to 22 (IQR 17.4-20.2), for instance.
#Let's visualise relationships (correlations) between variables in the data:
install.packages("corrplot")
library(corrplot)
cor_matrix <- cor(Boston)
corrplot(cor_matrix, method="circle", type="upper")
#The plot displays positive correlations in blue and negative correlations in red. Color intensity and the size of the circle are proportional to the correlation coefficients.
#Strongest positive correlations are found between accessibility to highways (rad) and property tax-rate (tax), and between air nitroxen oxide (nox) and industrial acres in town (indus).
#Strongest negative correlations are found between property value (medv) and proportion of low socioeconomic population (lstat), and between distance to employment center (dis) and air nitroxen oxide (nox).
# 4) Now let's standardize the dataset:
Boston_scaled <- scale(Boston)
summary(Boston_scaled)
#After standardizing the data, the mean for all variables is now 0.00, as expected.
#Let's convert the scaled data from a matrix format to a data frame format:
Boston_scaled <- as.data.frame(Boston_scaled)
#Let's create a categorical variable "crime" of the crime rate variable (crim):
summary(Boston_scaled$crim)
bins <- quantile(Boston_scaled$crim)
crime <- cut(Boston_scaled$crim, breaks = bins, include.lowest = TRUE, labels = c("low", "med_low", "med_high", "high"))
#Let's remove the crim-variable from the original data:
Boston_scaled <- dplyr::select(Boston_scaled, -crim)
dim(Boston_scaled)
#We now have 13 instead of 14 variables, good! Let's now add the new crime-variable to the scaled dataset:
Boston_scaled <- data.frame(Boston_scaled, crime)
dim(Boston_scaled)
str(Boston_scaled)
#...and we're back to 14 variables, with a categorical crime-variable, as can clearly be seen from analysing the structure of Boston_scaled.
#Let's divide the dataset to train (80%) and test (20%) sets:
nrow(Boston_scaled)
n <- nrow(Boston_scaled)
ind <- sample(n, size = n*0.8)
train <- Boston_scaled[ind, ]
test <- Boston_scaled[-ind, ]
# 5) Linear discriminant analysis:
lda.fit <- lda(crime ~ ., data = train)
lda.fit
#Drawing LDA biplot:
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "orange", tex = 0.75, choices = c(1,2)){
heads <- coef(x)
arrows(x0 = 0, y0 = 0,
x1 = myscale * heads[,choices[1]],
y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
text(myscale * heads[,choices], labels = row.names(heads),
cex = tex, col=color, pos=3)
}
classes <- as.numeric(train$crime)
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1)
# 6) Predicting classes using the LDA model on the test data:
#Let's first save the crime-categories from the test set as "correct_classes", so that we can compare the predicted results with them later on:
correct_classes <- test$crime
#Removing the crime-variable from the test dataset:
test <- dplyr::select(test, -crime)
#Predicting the classes and crosstabulating the results:
lda.pred <- predict(lda.fit, newdata = test)
table(correct = correct_classes, predicted = lda.pred$class)
#The table demonstrates (see below) that the LDA model predicted too low a class for 12 cases and too high a class for 23 cases. Most cases whose classes were wrongly predicted were in the "med_low" and "med_high" categories, but the predictive performance was quite good in the "low" and "high" categories (12/14 and 21/21 accurately predicted, respectively).
# 7) Clustering:
#Let's first reload the Boston dataset:
library(MASS)
data(Boston)
Boston2 <- Boston
#Standardizing the dataset:
Boston2_scaled <- scale(Boston2)
#Calculating (Euclidean) distances between observations:
dist_eu <- dist(Boston2_scaled)
summary(dist_eu)
#Running the kmeans algorithm, starting with 4 centers:
km <- kmeans(Boston2_scaled, centers = 4)
pairs(Boston2_scaled, col = km$cluster)
#This is all very well. However, let's now investigate what the optimal number of clusters is. Let's set the number of clusters a maximum of 10:
k_max <- 10
#Let's calculate the total within cluster sum of squares (twcss):
twcss <- sapply(1:k_max, function(k){kmeans(Boston, k)$tot.withinss})
#Let's visualize twcss:
twcss <- sapply(1:k_max, function(k){kmeans(Boston, k)$tot.withinss})
install.packages("ggplot2")
library(ggplot2)
qplot(x = 1:k_max, y = twcss, geom = "line")
#As we've learned, we know that the number of clusters is optimal when the twcss is at its smallest. Our plot clearly demonstrates that twcss drastically reduces when k=2, meaning that 2 clusters will serve us quite fine!
#Now we can re-run the kmeans algorithm:
km <- kmeans(Boston2_scaled, centers = 2)
pairs(Boston2_scaled, col = km$cluster)
#We end up with a rather complex-looking plot. Regrettably, I can't interpret this. Let's instead move on to the bonus-assignment:
# 8) BONUS:
#Standardizing data:
library(MASS)
data(Boston)
Boston3 <- Boston
Boston3_scaled <- scale(Boston3)
Boston3_scaled <- data.frame(Boston3_scaled)
#Running kmeans using 4 clusters:
km3 <- kmeans(Boston3_scaled, centers = 4)
#Performing LDA using clusters as target classes:
lda.fit3 <- lda(km3$cluster ~., data = Boston3_scaled)
#Visualisation:
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "orange", tex = 0.75, choices = c(1,2)){
heads <- coef(x)
arrows(x0 = 0, y0 = 0,
x1 = myscale * heads[,choices[1]],
y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
text(myscale * heads[,choices], labels = row.names(heads),
cex = tex, col=color, pos=3)
}
plot(lda.fit3, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit3, myscale = 1)
#Once again, we end up with quite a graph! Judging by arrow size, the variables "rad" and "indus" seem to be most influencial linear separators for the clusters.
install.packages("MASS")
install.packages("ggplot2")
install.packages(contrib.url)
View(Boston2_scaled)
View(lda.fit3)
install.packages("corrplot")
library(corrplot)
cor_matrix <- cor(Boston)
corrplot(cor_matrix, method="circle", type="upper")
plot
install.packages("corrplot")
install.packages("corrplot")
library(corrplot)
cor_matrix <- cor(Boston)
corrplot(cor_matrix, method="circle", type="upper")
hd <- read.csv("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/human_development.csv", stringsAsFactors = F)
hd <- read.csv("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/human_development.csv", stringsAsFactors = F)
gii <- read.csv("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/gender_inequality.csv", stringsAsFactors = F, na.strings = "..")
dim(hd)
str(hd)
dim(gii)
str(gii)
View(gii)
summary(hd)
summary(gii)
colnames(hd) <- c("HDI_rank", "country", "HDI", "exp_life", "exp_educ", "educ", "GNIpc", "GNIHDI_rank")
colnames(gii) <- c("GII_rank", "country", "GII", "mortality_maternal", "birth_adolescent", "female_parliament", "female_secondary", "male_secondary", "female_work", "male_work")
library(dplyr)
gii <- mutate(gii, sex_secondary = (female_secondary/male_secondary))
gii <- mutate(gii, sex_work = (female_work/male_work))
join_by <- "country"
hd_gii <- inner_join(hd, gii, by = join_by)
dim(hd_gii)
human <- hd_gii
write.table(human, file = "human")
---
title: "Week 5, analysis exercise"
author: "SM"
date: "28 November 2018"
output: html_document
---
```{}
#Here is week 5 analysis exercise! Let's take a look how it seems! :)
# I also downloaded the human_url from online, in case of my data wrangling was unsuccessful. I think it went right (right amount of variables and observations) but just to be sure.
human_url <- read.table("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/human2.txt", header = TRUE, sep = ",")
# 1) overview of data
summary(human_url)
Edu2.FM          Labo.FM          Edu.Exp         Life.Exp          GNI
Min.   :0.1717   Min.   :0.1857   Min.   : 5.40   Min.   :49.00   Min.   :   581
1st Qu.:0.7264   1st Qu.:0.5984   1st Qu.:11.25   1st Qu.:66.30   1st Qu.:  4198
Median :0.9375   Median :0.7535   Median :13.50   Median :74.20   Median : 12040
Mean   :0.8529   Mean   :0.7074   Mean   :13.18   Mean   :71.65   Mean   : 17628
3rd Qu.:0.9968   3rd Qu.:0.8535   3rd Qu.:15.20   3rd Qu.:77.25   3rd Qu.: 24512
Max.   :1.4967   Max.   :1.0380   Max.   :20.20   Max.   :83.50   Max.   :123124
Mat.Mor         Ado.Birth         Parli.F
Min.   :   1.0   Min.   :  0.60   Min.   : 0.00
1st Qu.:  11.5   1st Qu.: 12.65   1st Qu.:12.40
Median :  49.0   Median : 33.60   Median :19.30
Mean   : 149.1   Mean   : 47.16   Mean   :20.91
3rd Qu.: 190.0   3rd Qu.: 71.95   3rd Qu.:27.95
Max.   :1100.0   Max.   :204.80   Max.   :57.50
# As we can see, GNI ranges from 581 to 123124. Also, we can see that women has almost as often completed secondary education as men.
#Lets  take a look about correlations between varibles
install.packages("corrplot")
library(corrplot)
cor_matrix <- cor(human_url)
corrplot(cor_matrix, method="circle", type="upper")
#As we can see, there is a strong negative correlation between maternal mortality and life expectancy, which is hardly surprising. Moreover, there are relatively strong positive correlations between maternal mortality and adolescent birth rate, and between life expectancy and expected years of education.
# 2) Principal component analysis (PCA)
pca_human_url <- prcomp(human_url)
pca_human_url
Rotation (n x k) = (8 x 8):
PC1           PC2           PC3           PC4           PC5           PC6
Edu2.FM   -5.607472e-06  0.0006713951 -3.412027e-05 -2.736326e-04 -0.0022935252  2.180183e-02
Labo.FM    2.331945e-07 -0.0002819357  5.302884e-04 -4.692578e-03  0.0022190154  3.264423e-02
Edu.Exp   -9.562910e-05  0.0075529759  1.427664e-02 -3.313505e-02  0.1431180282  9.882477e-01
Life.Exp  -2.815823e-04  0.0283150248  1.294971e-02 -6.752684e-02  0.9865644425 -1.453515e-01
GNI       -9.999832e-01 -0.0057723054 -5.156742e-04  4.932889e-05 -0.0001135863 -2.711698e-05
Mat.Mor    5.655734e-03 -0.9916320120  1.260302e-01 -6.100534e-03  0.0266373214  1.695203e-03
Ado.Birth  1.233961e-03 -0.1255502723 -9.918113e-01  5.301595e-03  0.0188618600  1.273198e-02
Parli.F   -5.526460e-05  0.0032317269 -7.398331e-03 -9.971232e-01 -0.0716401914 -2.309896e-02
PC7           PC8
Edu2.FM    6.998623e-01  7.139410e-01
Labo.FM    7.132267e-01 -7.001533e-01
Edu.Exp   -3.826887e-02  7.776451e-03
Life.Exp   5.380452e-03  2.281723e-03
GNI       -8.075191e-07 -1.176762e-06
Mat.Mor    1.355518e-04  8.371934e-04
Ado.Birth -8.641234e-05 -1.707885e-04
Parli.F   -2.642548e-03  2.680113e-03
#And a plot about that!
biplot(pca_human_url, choices = 1:2, cex = c(0.8, 1), col = c("grey40", "deeppink2"))
# 3) Repeating the analysis with standardized variables
human_std <- scale(human_url)
summary(human_std)
Edu2.FM           Labo.FM           Edu.Exp           Life.Exp            GNI
Min.   :-2.8189   Min.   :-2.6247   Min.   :-2.7378   Min.   :-2.7188   Min.   :-0.9193
1st Qu.:-0.5233   1st Qu.:-0.5484   1st Qu.:-0.6782   1st Qu.:-0.6425   1st Qu.:-0.7243
Median : 0.3503   Median : 0.2316   Median : 0.1140   Median : 0.3056   Median :-0.3013
Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000
3rd Qu.: 0.5958   3rd Qu.: 0.7350   3rd Qu.: 0.7126   3rd Qu.: 0.6717   3rd Qu.: 0.3712
Max.   : 2.6646   Max.   : 1.6632   Max.   : 2.4730   Max.   : 1.4218   Max.   : 5.6890
Mat.Mor          Ado.Birth          Parli.F
Min.   :-0.6992   Min.   :-1.1325   Min.   :-1.8203
1st Qu.:-0.6496   1st Qu.:-0.8394   1st Qu.:-0.7409
Median :-0.4726   Median :-0.3298   Median :-0.1403
Mean   : 0.0000   Mean   : 0.0000   Mean   : 0.0000
3rd Qu.: 0.1932   3rd Qu.: 0.6030   3rd Qu.: 0.6127
Max.   : 4.4899   Max.   : 3.8344   Max.   : 3.1850
# And a plot
pca_human_std <- prcomp(human_std)
biplot(pca_human_std, choices = 1:2, cex = c(0.8, 1), col = c("grey40", "deeppink2"))
# Lets intepret the results now. As we can see, before standardization, the GNI variable affects so much that the other variables seems to be stacked. From the summary of unstandardized version we can see that the GNI has more than 100 times greater effect to PC1 than other coefficients.
# In the standardized data, it appears that the PC1 is affected by factors such as life expectancy, educational factors and on the other hand, maternal mortality and adolecsent birth rate. It seems that in PC2 case, there is effect to PC2 from two factors: Women in working life and amount of women in parliament.
# 4)
# So, my personal interpretations. PC1:  It seems that we can divide countries using these factors. The third world countries have more maternal mortality and adolecsent birth rate. Western countries have more life expectancy and educational factors. PC2: women presence in working life and parliament seems to be very low in islamic countries, such as Yemen, Iran and Arab Republic.
# 5) Multiple correspondence analysis
install.packages("FactoMineR")
library(FactoMineR)
data(tea)
str(tea)
dim(tea)
# So, the tea data contains 300 observations and 36 variables. Quite of few of a variables seems to be categorical. Here's a first row of summary of the variables:
summary(tea)
breakfast           tea.time          evening          lunch            dinner
breakfast    :144   Not.tea time:131   evening    :103   lunch    : 44   dinner    : 21
Not.breakfast:156   tea time    :169   Not.evening:197   Not.lunch:256   Not.dinner:279
# Visualization
install.packages("dplyr")
install.packages("tidyr")
install.packages("ggplot2")
library(dplyr)
library(tidyr)
library(ggplot2)
gather(tea) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_bar() + theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8))
# Let's visualize only few multinomial variables:
keep_columns <- c("Tea", "How", "how", "sugar", "where", "lunch")
tea_time <- select(tea, one_of(keep_columns))
summary(tea_time)
str(tea_time)
gather(tea_time) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_bar() + theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8))
# so, as we can see, people like to drink tea mostly when its in tea bag. Also, tea alone is the most popular opinion. And according to data, tea doesn't belong to lunch. Earl grey is the most popular tea and chain store is the place to be.
# multiple correspondence analysis
mca <- MCA(tea_time, graph = FALSE)
summary(mca)
Call:
MCA(X = tea_time, graph = FALSE)
Eigenvalues
Dim.1   Dim.2   Dim.3   Dim.4   Dim.5   Dim.6   Dim.7   Dim.8   Dim.9
Variance               0.279   0.261   0.219   0.189   0.177   0.156   0.144   0.141   0.117
% of var.             15.238  14.232  11.964  10.333   9.667   8.519   7.841   7.705   6.392
Cumulative % of var.  15.238  29.471  41.435  51.768  61.434  69.953  77.794  85.500  91.891
Dim.10  Dim.11
Variance               0.087   0.062
% of var.              4.724   3.385
Cumulative % of var.  96.615 100.000
Individuals (the 10 first)
Dim.1    ctr   cos2    Dim.2    ctr   cos2    Dim.3    ctr   cos2
1                  | -0.298  0.106  0.086 | -0.328  0.137  0.105 | -0.327  0.163  0.104 |
2                  | -0.237  0.067  0.036 | -0.136  0.024  0.012 | -0.695  0.735  0.314 |
3                  | -0.369  0.162  0.231 | -0.300  0.115  0.153 | -0.202  0.062  0.069 |
4                  | -0.530  0.335  0.460 | -0.318  0.129  0.166 |  0.211  0.068  0.073 |
5                  | -0.369  0.162  0.231 | -0.300  0.115  0.153 | -0.202  0.062  0.069 |
6                  | -0.369  0.162  0.231 | -0.300  0.115  0.153 | -0.202  0.062  0.069 |
7                  | -0.369  0.162  0.231 | -0.300  0.115  0.153 | -0.202  0.062  0.069 |
8                  | -0.237  0.067  0.036 | -0.136  0.024  0.012 | -0.695  0.735  0.314 |
9                  |  0.143  0.024  0.012 |  0.871  0.969  0.435 | -0.067  0.007  0.003 |
10                 |  0.476  0.271  0.140 |  0.687  0.604  0.291 | -0.650  0.643  0.261 |
Categories (the 10 first)
Dim.1     ctr    cos2  v.test     Dim.2     ctr    cos2  v.test     Dim.3
black              |   0.473   3.288   0.073   4.677 |   0.094   0.139   0.003   0.929 |  -1.081
Earl Grey          |  -0.264   2.680   0.126  -6.137 |   0.123   0.626   0.027   2.867 |   0.433
green              |   0.486   1.547   0.029   2.952 |  -0.933   6.111   0.107  -5.669 |  -0.108
alone              |  -0.018   0.012   0.001  -0.418 |  -0.262   2.841   0.127  -6.164 |  -0.113
lemon              |   0.669   2.938   0.055   4.068 |   0.531   1.979   0.035   3.226 |   1.329
milk               |  -0.337   1.420   0.030  -3.002 |   0.272   0.990   0.020   2.422 |   0.013
other              |   0.288   0.148   0.003   0.876 |   1.820   6.347   0.102   5.534 |  -2.524
tea bag            |  -0.608  12.499   0.483 -12.023 |  -0.351   4.459   0.161  -6.941 |  -0.065
tea bag+unpackaged |   0.350   2.289   0.056   4.088 |   1.024  20.968   0.478  11.956 |   0.019
unpackaged         |   1.958  27.432   0.523  12.499 |  -1.015   7.898   0.141  -6.482 |   0.257
ctr    cos2  v.test
black               21.888   0.382 -10.692 |
Earl Grey            9.160   0.338  10.053 |
green                0.098   0.001  -0.659 |
alone                0.627   0.024  -2.655 |
lemon               14.771   0.218   8.081 |
milk                 0.003   0.000   0.116 |
other               14.526   0.197  -7.676 |
tea bag              0.183   0.006  -1.287 |
tea bag+unpackaged   0.009   0.000   0.226 |
unpackaged           0.602   0.009   1.640 |
Categorical variables (eta2)
Dim.1 Dim.2 Dim.3
Tea                | 0.126 0.108 0.410 |
How                | 0.076 0.190 0.394 |
how                | 0.708 0.522 0.010 |
sugar              | 0.065 0.001 0.336 |
where              | 0.702 0.681 0.055 |
lunch              | 0.000 0.064 0.111 |
# and visualization of that!
plot(mca, invisible=c("ind"), habillage = "quali")
# Comments are pretty same as above.
# THE END!
```
```
View(human_url)
spearman.ci(GNI, Mat.Mor, nrep = 1000, conf.level = 0.95)
install.packages("pspearman")
library(spearman)
library(pspearman)
spearman.ci(GNI, Mat.Mor, nrep = 1000, conf.level = 0.95)
spearman.test(GNI, Mat.Mor)
