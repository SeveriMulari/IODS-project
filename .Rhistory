colnames(learning2014)[2] <- "age"
colnames(learning2014)[7] <- "points"
colnames(learning2014)
male_students <- filter(learning2014, gender == "M")
learning2014 <- filter(learning2014, points > 0)
View(learning2014)
library(ggplot2)
library(ggplot2)
p1 <- ggplot(learning2014, aes(x = attitude, y = points, col = gender))
library(ggplot2)
library(ggplot2)
library(ggplot2)
install.packages("ggplot2")
library(ggplot2)
str(learning2014)
dim(learning2014)
library(ggplot2)
p1 <- ggplot(learning2014, aes(x = attitude, y = points, col = gender))
View(male_students)
View(p1)
p2 <- p1 + geom_point()
p2
p3 <- p2 + geom_smooth(method = "lm")
p4 <- p3 + ggtitle("Student's attitude versus exam points")
p4
pairs(learning2014[-1], col = learning2014$gender)
library(GGally)
install.packages(ggally)
install.packages(GGally)
library(ggally)
install.packages("GGally")
library(GGally)
p <- ggpairs(learning2014, mapping = aes(col = gender, alpha = 0.3), lower = list(combo = wrap("facethist", bins = 20)))
p
ggpairs(learning2014, lower = list(combo = wrap("facethist", bins = 20)))
my_model2 <- lm(points ~ attitude + stra + surf, data = learning2014)
summary(my_model2)
my_model11 <- lm(points ~ attitude, data = learning2014)
summary(my_model11)
par(mfrow = c(2,2))
plot(my_model2, which = c(1,2,5))
url <- "http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets"
url_math <- paste(url, "student-mat.csv", sep = "/")
url_math
math <- read.table(url_math, sep = ";" , header=TRUE)
url_por <- paste(url, "student-por.csv", sep ="/")
url_por
por <- read.table(url_por, sep = ";", header = TRUE)
por <- read.table(url_por, sep = ";", header = TRUE)
por <- read.table(url_por, sep = ";", header = TRUE)
por <- read.table(url_por, sep = ";", header = TRUE)
colnames(math)
colnames(por)
str(math)
str(por)
dim(math)
dim(por)
library(dplyr)
join_by <- c("school","sex","age","address","famsize","Pstatus","Medu","Fedu","Mjob","Fjob","reason","nursery","internet")
math_por <- inner_join(math, por, by = join_by, suffix = c(".math", ".por"))
colnames(math_por)
glimpse(math_por)
dim(math_por)
str(math_por)
alc <- select(math_por, one_of(join_by))
notjoined_columns <- colnames(math)[!colnames(math) %in% join_by]
notjoined_columns
read.csv("C:\Users\sever\OneDrive\Desktop\IODS-project\data\student-mat.csv")
read.csv("C:\\Users\\sever\\OneDrive\\Desktop\\IODS-project\\data\\student-mat.csv")
read.csv("C:\\Users\\sever\\OneDrive\\Desktop\\IODS-project\\data\\student-mat.csv", sep = ";", header = TRUE)
data_mat <- read.csv("C:\\Users\\sever\\OneDrive\\Desktop\\IODS-project\\data\\student-mat.csv", sep = ";", header = TRUE)
data_por <- read.csv("C:\\Users\\sever\\OneDrive\\Desktop\\IODS-project\\data\\student-por.csv", sep = ";", header = TRUE)
View(alc)
View(por)
data_mat <- read.csv("C:\\Users\\sever\\OneDrive\\Desktop\\IODS-project\\data\\student-mat.csv", sep = ";", header = TRUE)
data_por <- read.csv("C:\\Users\\sever\\OneDrive\\Desktop\\IODS-project\\data\\student-por.csv", sep = ";", header = TRUE)
str(data_por)
str(data_mat)
dim(data_por)
dim(data_mat)
summary(data_mat)
summary(data_por)
library(dplyr)
join_byjoin_by <- c("school","sex","age","address","famsize","Pstatus","Medu","Fedu","Mjob","Fjob","reason","nursery","internet") <- c("school","sex","age","address","famsize","Pstatus","Medu","Fedu","Mjob","Fjob","reason","nursery","internet")
join_by <- c("school","sex","age","address","famsize","Pstatus","Medu","Fedu","Mjob","Fjob","reason","nursery","internet") <- c("school","sex","age","address","famsize","Pstatus","Medu","Fedu","Mjob","Fjob","reason","nursery","internet")
join_by <- c("school","sex","age","address","famsize","Pstatus","Medu","Fedu","Mjob","Fjob","reason","nursery","internet")
inner_join(math, por, by = join_by, suffix = c(".math", ".por"))
mat_por <- inner_join(math, por, by = join_by, suffix = c(".math", ".por"))
mat_por <- inner_join(mat, por, by = join_by, suffix = c(".math", ".por"))
mat_por <- inner_join(mat, por, by = join_by, suffix = c(".mat", ".por"))
mat_por <- inner_join(data_mat, data_por, by = join_by, suffix = c(".data_mat", ".data_por"))
View(data_por)
str(mat_por)
dim(mat_por)
alc <- select(math_por, one_of(join_by))
alc <- select(mat_por, one_of(join_by))
notjoined_columns <- colnames(math)[!colnames(math) %in% join_by]
notjoined_columns <- colnames(math)[!colnames(mat) %in% join_by]
notjoined_columns <- colnames(mat)[!colnames(mat) %in% join_by]
notjoined_columns <- colnames(data_mat)[!colnames(mat) %in% join_by]
notjoined_columns <- colnames(data_mat)[!colnames(data_mat) %in% join_by]
notjoined_columns
for(column_name in notjoined_columns) {
two_columns <- select(math_por, starts_with(column_name))
first_column <- select(two_columns, 1)[[1]]
if(is.numeric(first_column)) {
alc[column_name] <- round(rowMeans(two_columns))
} else {
alc[column_name] <- first_column
}
}
for(column_name in notjoined_columns) {
two_columns <- select(mat_por, starts_with(column_name))
first_column <- select(two_columns, 1)[[1]]
if(is.numeric(first_column)) {
alc[column_name] <- round(rowMeans(two_columns))
} else {
alc[column_name] <- first_column
}
}
glimpse(alc)
alc <- select(mat_por, one_of(join_by))
notjoined_columns <- colnames(data_mat)[!colnames(data_mat) %in% join_by]
notjoined_columns
for(column_name in notjoined_columns) {
two_columns <- select(mat_por, starts_with(column_name))
first_column <- select(two_columns, 1)[[1]]
if(is.numeric(first_column)) {
alc[column_name] <- round(rowMeans(two_columns))
} else {
alc[column_name] <- first_column
}
}
alc <- notjoined_columns
for(column_name in notjoined_columns) {
two_columns <- select(mat_por, starts_with(column_name))
first_column <- select(two_columns, 1)[[1]]
if(is.numeric(first_column)) {
alc[column_name] <- round(rowMeans(two_columns))
} else {
alc[column_name] <- first_column
}
}
alc <- select(mat_por, one_of(join_by))
notjoined_columns <- colnames(data_mat)[!colnames(data_mat) %in% join_by]
notjoined_columns
for(column_name in notjoined_columns) {
two_columns <- select(mat_por, starts_with(column_name))
first_column <- select(two_columns, 1)[[1]]
if(is.numeric(first_column)) {
alc[column_name] <- round(rowMeans(two_columns))
} else {
alc[column_name] <- first_column
}
}
library(dplyr); library(ggplot2)
alc <- mutate(alc, alc_use = (Dalc + Walc) / 2)
alc %>% group_by(sex, high_use) %>% summarise(count = n(), mean_grade = mean(G3))
alc <- mutate(alc, high_use = alc_use > 2)
glimpse(alc)
write.csv(create_alc)
write.csv(alc)
write.table(alc, file = create_alc)
write.table(alc, file = "create_alc")
write.table(alc, file = "create_alc2")
```{r child = "chapter1.Rmd"}
```
***
```{r child = "chapter2.Rmd"}
```
***
```{r child = "chapter3.Rmd"}
```
***
data <- read.csv("C:\\Users\\sever\\OneDrive\\Desktop\\IODS-project\\data\\create_alc.R, sep = ;", header = TRUE)
data <- read.csv("C:\Users\sever\OneDrive\Desktop\IODS-project\data\create_alc, sep = ";", header = TRUE)
data <- read.csv("C:\\Users\\sever\\OneDrive\\Desktop\\IODS-project\\data\\create_alc, sep = ";", header = TRUE)
data <- read.csv("C:\\Users\\sever\\OneDrive\\Desktop\\IODS-project\\data\\create_alc, header = TRUE)
data <- read.csv("C:\\Users\\sever\\OneDrive\\Desktop\\IODS-project\\data\\create_alc, header = TRUE)
data
data
data <- read.csv("C:\\Users\\sever\\OneDrive\\Desktop\\IODS-project\\data\\create_alc, header = TRUE, sep = " ")
data
data
data <- read.table("C:\\Users\\sever\\OneDrive\\Desktop\\IODS-project\\data\\create_alc, header = TRUE, sep = " ")
data <- read.table("C:\\Users\\sever\\OneDrive\\Desktop\\IODS-project\\data\\create_alc, header = TRUE, sep = " ")
data
data
data <- read.table("C:\\Users\\sever\\OneDrive\\Desktop\\IODS-project\\data\\create_alc, header = TRUE)
data <- read.table("C:\\Users\\sever\\OneDrive\\Desktop\\IODS-project\\data\\create_alc, header = TRUE")
write.csv(alc, file = ALC_Excel)
write.csv(alc, file = ALC_Excel)
write.table(alc, file = "create_alc11")
create_alc <- read.csv("C:/Users/sever/OneDrive/Desktop/IODS-project/data/create_alc.R", sep=";", comment.char="#")
View(create_alc)
View(alc)
View(alc)
data <- read.table("C:\Users\sever\OneDrive\Desktop\IODS-project\data\create_alc", header = TRUE)
data <- read.table("C:\\Users\\sever\\OneDrive\\Desktop\\IODS-project\\data\\create_alc", header = TRUE)
View(data)
dim(data)
colnames(data)
summary(data)
hist(data$age)
hist(data$sex)
hist(data$sex)
hist(data$guardian)
hist(data$health)
alc %>% group_by(sex, high_use) %>% summarise(count = n(), mean_grade = mean(G3))
summary(data$age)
summary(data$sex)
summary(data$guardian)
summary(data$health)
install.packages("gmodels")
library(gmodels)
g1 <- ggplot(alc, aes(x = high_use, y = G3, col = sex))
g1 + geom_boxplot() + ylab("grade")
g2 <- ggplot(alc, aes(x = high_use, y = absences, col = sex))
g2 + geom_boxplot() + ggtitle("Student absences by alcohol consumption and sex")
crosstab(data$age, data$high_use)
crosstable(data$age, data$high_use)
CrossTable(data$age, data$high_use)
CrossTable(data$sex, data$high_use)
CrossTable(data$guardian, data$high_use)
CrossTable(data$health, data$high_use)
median(age)
median(data$age)
CrossTable(median(data$age), data$high_use)
model <- glm(high_use ~ age + guardian + sex + health, data = data, family = "binomial")
model
summary(model)
OR <- coef(model) %>% exp
OR <- coef(model) %>% exp
CI <- confint(model) %>% exp
cbind(OR, CI)
model <- glm(high_use ~ age + guardian + sex + health, data = data, family)
model
summary(model)
# Age and sexM had p-values < 0.05 and have thus statistical significance.
OR <- coef(model) %>% exp
CI <- confint(model) %>% exp
cbind(OR, CI)
model <- glm(high_use ~ age + sex + guardian + health, data = data)
model
summary(model)
# Age and sexM had p-values < 0.05 and have thus statistical significance.
OR <- coef(model) %>% exp
CI <- confint(model) %>% exp
cbind(OR, CI)
probabilities <- predict(model, type = "response")
data <- mutate(data, probability = probabilities)
data <- mutate(data, prediction = probability > 0.5)
select(data, age, sex, guardian, health, high_use, probability, prediction) %>% tail(10)
table(high_use = data$high_use, prediction = data$prediction)
View(g1)
View(two_columns)
probabilities <- predict(model, type = "response")
data <- mutate(data, probability = probabilities)
data <- mutate(data, prediction = probability > 0.5)
select(data, age, sex, high_use, probability, prediction) %>% tail(10)
table(high_use = data$high_use, prediction = data$prediction)
OR <- coef(model) %>% exp
CI <- confint(model) %>% exp
cbind(OR, CI)
install.packages("MASS")
library(MASS)
data(Boston)
str(Boston)
dim(Boston)
summary(Boston)
install.packages("corrplot")
library(corrplot)
cor_matrix <- cor(Boston)
corrplot(cor_matrix, method="circle", type="upper")
corrplot
corrplot(cor_matrix, method="circle", type="upper")
View(cor_matrix)
Boston_scaled <- scale(Boston)
summary(Boston_scaled)
Boston_scaled <- as.data.frame(Boston_scaled)
summary(Boston_scaled$crim)
bins <- quantile(Boston_scaled$crim)
crime <- cut(Boston_scaled$crim, breaks = bins, include.lowest = TRUE, labels = c("low", "med_low", "med_high", "high"))
Boston_scaled <- dplyr::select(Boston_scaled, -crim)
dim(Boston_scaled)
Boston_scaled <- data.frame(Boston_scaled, crime)
dim(Boston_scaled)
str(Boston_scaled)
nrow(Boston_scaled)
n <- nrow(Boston_scaled)
ind <- sample(n, size = n*0.8)
train <- Boston_scaled[ind, ]
test <- Boston_scaled[-ind, ]
lda.fit <- lda(crime ~ ., data = train)
lda.fit
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "orange", tex = 0.75, choices = c(1,2)){
heads <- coef(x)
arrows(x0 = 0, y0 = 0,
x1 = myscale * heads[,choices[1]],
y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
text(myscale * heads[,choices], labels = row.names(heads),
cex = tex, col=color, pos=3)
}
classes <- as.numeric(train$crime)
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1)
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "orange", tex = 0.75, choices = c(1,2)){
heads <- coef(x)
arrows(x0 = 0, y0 = 0,
x1 = myscale * heads[,choices[1]],
y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
text(myscale * heads[,choices], labels = row.names(heads),
cex = tex, col=color, pos=3)
}
classes <- as.numeric(train$crime)
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1)
View(lda.fit)
correct_classes <- test$crime
test <- dplyr::select(test, -crime)
lda.pred <- predict(lda.fit, newdata = test)
table(correct = correct_classes, predicted = lda.pred$class)
library(MASS)
data(Boston)
Boston2 <- Boston
Boston2_scaled <- scale(Boston2)
dist_eu <- dist(Boston2_scaled)
summary(dist_eu)
km <- kmeans(Boston2_scaled, centers = 4)
pairs(Boston2_scaled, col = km$cluster)
k_max <- 10
twcss <- sapply(1:k_max, function(k){kmeans(Boston, k)$tot.withinss})
twcss <- sapply(1:k_max, function(k){kmeans(Boston, k)$tot.withinss})
twcss <- sapply(1:k_max, function(k){kmeans(Boston, k)$tot.withinss})
install.packages("ggplot2")
library(ggplot2)
qplot(x = 1:k_max, y = twcss, geom = "line")
p1
km <- kmeans(Boston2_scaled, centers = 2)
pairs(Boston2_scaled, col = km$cluster)
library(MASS)
data(Boston)
Boston3 <- Boston
Boston3_scaled <- scale(Boston3)
Boston3_scaled <- data.frame(Boston3_scaled)
km3 <- kmeans(Boston3_scaled, centers = 4)
lda.fit3 <- lda(km3$cluster ~., data = Boston3_scaled)
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "orange", tex = 0.75, choices = c(1,2)){
heads <- coef(x)
arrows(x0 = 0, y0 = 0,
x1 = myscale * heads[,choices[1]],
y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
text(myscale * heads[,choices], labels = row.names(heads),
cex = tex, col=color, pos=3)
}
plot(lda.fit3, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit3, myscale = 1)
View(lda.arrows)
<Anonymous> ... <Anonymous> -> parse_all -> parse_all.character -> parse
knitr::opts_chunk$set(echo = TRUE)
# Here is exercise 4; clustering and classification analysis exercise!
# 2) Let's first download the Boston data:
install.packages("MASS")
library(MASS)
data(Boston)
str(Boston)
dim(Boston)
#The dataset includes 506 observations and 14 variables. The data contains housing data from the Boston area, including variables such as crime rate (crim), distance to employment centers (dis), and pupil-teacher ratio (ptratio).
# 3) Let's have a look at a summary of the included variables:
summary(Boston)
#The summary suggests that crime rates range from 0.00632 to 88.97620 per capita (IQR 0.08-3.67), and that the pupil-teacher ratio ranges from 12.6 to 22 (IQR 17.4-20.2), for instance.
#Let's visualise relationships (correlations) between variables in the data:
install.packages("corrplot")
library(corrplot)
cor_matrix <- cor(Boston)
corrplot(cor_matrix, method="circle", type="upper")
#The plot displays positive correlations in blue and negative correlations in red. Color intensity and the size of the circle are proportional to the correlation coefficients.
#Strongest positive correlations are found between accessibility to highways (rad) and property tax-rate (tax), and between air nitroxen oxide (nox) and industrial acres in town (indus).
#Strongest negative correlations are found between property value (medv) and proportion of low socioeconomic population (lstat), and between distance to employment center (dis) and air nitroxen oxide (nox).
# 4) Now let's standardize the dataset:
Boston_scaled <- scale(Boston)
summary(Boston_scaled)
#After standardizing the data, the mean for all variables is now 0.00, as expected.
#Let's convert the scaled data from a matrix format to a data frame format:
Boston_scaled <- as.data.frame(Boston_scaled)
#Let's create a categorical variable "crime" of the crime rate variable (crim):
summary(Boston_scaled$crim)
bins <- quantile(Boston_scaled$crim)
crime <- cut(Boston_scaled$crim, breaks = bins, include.lowest = TRUE, labels = c("low", "med_low", "med_high", "high"))
#Let's remove the crim-variable from the original data:
Boston_scaled <- dplyr::select(Boston_scaled, -crim)
dim(Boston_scaled)
#We now have 13 instead of 14 variables, good! Let's now add the new crime-variable to the scaled dataset:
Boston_scaled <- data.frame(Boston_scaled, crime)
dim(Boston_scaled)
str(Boston_scaled)
#...and we're back to 14 variables, with a categorical crime-variable, as can clearly be seen from analysing the structure of Boston_scaled.
#Let's divide the dataset to train (80%) and test (20%) sets:
nrow(Boston_scaled)
n <- nrow(Boston_scaled)
ind <- sample(n, size = n*0.8)
train <- Boston_scaled[ind, ]
test <- Boston_scaled[-ind, ]
# 5) Linear discriminant analysis:
lda.fit <- lda(crime ~ ., data = train)
lda.fit
#Drawing LDA biplot:
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "orange", tex = 0.75, choices = c(1,2)){
heads <- coef(x)
arrows(x0 = 0, y0 = 0,
x1 = myscale * heads[,choices[1]],
y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
text(myscale * heads[,choices], labels = row.names(heads),
cex = tex, col=color, pos=3)
}
classes <- as.numeric(train$crime)
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1)
# 6) Predicting classes using the LDA model on the test data:
#Let's first save the crime-categories from the test set as "correct_classes", so that we can compare the predicted results with them later on:
correct_classes <- test$crime
#Removing the crime-variable from the test dataset:
test <- dplyr::select(test, -crime)
#Predicting the classes and crosstabulating the results:
lda.pred <- predict(lda.fit, newdata = test)
table(correct = correct_classes, predicted = lda.pred$class)
#The table demonstrates (see below) that the LDA model predicted too low a class for 12 cases and too high a class for 23 cases. Most cases whose classes were wrongly predicted were in the "med_low" and "med_high" categories, but the predictive performance was quite good in the "low" and "high" categories (12/14 and 21/21 accurately predicted, respectively).
# 7) Clustering:
#Let's first reload the Boston dataset:
library(MASS)
data(Boston)
Boston2 <- Boston
#Standardizing the dataset:
Boston2_scaled <- scale(Boston2)
#Calculating (Euclidean) distances between observations:
dist_eu <- dist(Boston2_scaled)
summary(dist_eu)
#Running the kmeans algorithm, starting with 4 centers:
km <- kmeans(Boston2_scaled, centers = 4)
pairs(Boston2_scaled, col = km$cluster)
#This is all very well. However, let's now investigate what the optimal number of clusters is. Let's set the number of clusters a maximum of 10:
k_max <- 10
#Let's calculate the total within cluster sum of squares (twcss):
twcss <- sapply(1:k_max, function(k){kmeans(Boston, k)$tot.withinss})
#Let's visualize twcss:
twcss <- sapply(1:k_max, function(k){kmeans(Boston, k)$tot.withinss})
install.packages("ggplot2")
library(ggplot2)
qplot(x = 1:k_max, y = twcss, geom = "line")
#As we've learned, we know that the number of clusters is optimal when the twcss is at its smallest. Our plot clearly demonstrates that twcss drastically reduces when k=2, meaning that 2 clusters will serve us quite fine!
#Now we can re-run the kmeans algorithm:
km <- kmeans(Boston2_scaled, centers = 2)
pairs(Boston2_scaled, col = km$cluster)
#We end up with a rather complex-looking plot. Regrettably, I can't interpret this. Let's instead move on to the bonus-assignment:
# 8) BONUS:
#Standardizing data:
library(MASS)
data(Boston)
Boston3 <- Boston
Boston3_scaled <- scale(Boston3)
Boston3_scaled <- data.frame(Boston3_scaled)
#Running kmeans using 4 clusters:
km3 <- kmeans(Boston3_scaled, centers = 4)
#Performing LDA using clusters as target classes:
lda.fit3 <- lda(km3$cluster ~., data = Boston3_scaled)
#Visualisation:
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "orange", tex = 0.75, choices = c(1,2)){
heads <- coef(x)
arrows(x0 = 0, y0 = 0,
x1 = myscale * heads[,choices[1]],
y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
text(myscale * heads[,choices], labels = row.names(heads),
cex = tex, col=color, pos=3)
}
plot(lda.fit3, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit3, myscale = 1)
#Once again, we end up with quite a graph! Judging by arrow size, the variables "rad" and "indus" seem to be most influencial linear separators for the clusters.
install.packages("MASS")
install.packages("ggplot2")
install.packages(contrib.url)
View(Boston2_scaled)
View(lda.fit3)
install.packages("corrplot")
library(corrplot)
cor_matrix <- cor(Boston)
corrplot(cor_matrix, method="circle", type="upper")
plot
install.packages("corrplot")
install.packages("corrplot")
library(corrplot)
cor_matrix <- cor(Boston)
corrplot(cor_matrix, method="circle", type="upper")
hd <- read.csv("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/human_development.csv", stringsAsFactors = F)
hd <- read.csv("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/human_development.csv", stringsAsFactors = F)
gii <- read.csv("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/gender_inequality.csv", stringsAsFactors = F, na.strings = "..")
dim(hd)
str(hd)
dim(gii)
str(gii)
View(gii)
summary(hd)
summary(gii)
colnames(hd) <- c("HDI_rank", "country", "HDI", "exp_life", "exp_educ", "educ", "GNIpc", "GNIHDI_rank")
colnames(gii) <- c("GII_rank", "country", "GII", "mortality_maternal", "birth_adolescent", "female_parliament", "female_secondary", "male_secondary", "female_work", "male_work")
library(dplyr)
gii <- mutate(gii, sex_secondary = (female_secondary/male_secondary))
gii <- mutate(gii, sex_work = (female_work/male_work))
join_by <- "country"
hd_gii <- inner_join(hd, gii, by = join_by)
dim(hd_gii)
human <- hd_gii
write.table(human, file = "human")
